---
title: "How how to do a simple prediction problem using ridgereg() function"
author: "Duc Tran and William Wiik"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    fig_caption: yes
    toc: yes
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{How how to do a simple prediction problem using ridgereg() function}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


# Introduction

This vignette show you how to do a simple prediction problem using 
`ridgereg()` function.
We are going to use the `caret` package and `ridgereg()` function to create a predictive model for the BostonHousing data found in the `mlbench` package.

# 1.2

## 1

First we divide the BostonHousing data into a test and training dataset using the
`caret` package (with the proportions 25% and 75%).


```{r}
library(caret)
library(mlbench)
library(dplyr)
library(bonuslabpackage)
library(leaps)

data(BostonHousing)
# Set the seed for reproducibility
set.seed(123)

training_indices <- createDataPartition(BostonHousing$crim, p = 0.75, list = FALSE)

train_set <- BostonHousing[training_indices, ]
test_set <- BostonHousing[-training_indices, ]


```


## 2

We fit a linear regression model with all predictor variables and a fit a linear regression model with forward selection of covariates on the training dataset.


```{r}
# Linear model with all predictor variables
lin_mod <- lm(crim ~ ., data = train_set)


# Linear model with only intercept
model_intercept <- lm("crim ~ 1", data = train_set) 

# Forward selection algorithm for linear model.
test_forward <- train(crim ~ ., data = train_set, 
                      method = "leapForward",
                      tuneGrid = data.frame(nvmax = 1:13)) # max all variables

# nvmax shows that 12 predictor variables were the max.
test_forward
```


The forward selection algorithm shows that 12 predictor variables is the best.
The variables used and in what order are presented below.

```{r}
# Algorithm shows that the best model is to add predictor in the order as follows:
# rad, lstat, b, and zn.
summary(test_forward)
```


The best model is to add the predictor variables in the order: rad, lstat, b, zn, dis, indus, medv, nox, ptratio, chas, tax, and age. The only variable not used is rm.
We estimate this linear model and compare it with the model with only the intercept by using AIC.

```{r}
formula <- as.formula("crim ~ rad + lstat + b + zn + dis + indus + 
                    medv + nox + ptratio + chas + tax + age")

# Forward-selection model used.
forward_mod <- lm(formula, train_set, method="lm")

# We use AIC to compare the model with only intercept with the forward-selection model.
AIC(model_intercept)
AIC(forward_mod)

```

From the output the AIC for the forward selection model is lower than the model with intercept.
This implies that the forward selection model is better with AIC as a criterion.


## 3

Now we evaluate the performance of this model on the training dataset.

```{r}


# Predict using the selected model on the training dataset
train_predictions <- predict(forward_mod, newdata = train_set)


mae <- mean(abs(train_set$crim - train_predictions))
mse <- mean((train_set$crim - train_predictions)^2)
rmse <- sqrt(mse)
rsquared <- cor(train_set$crim, train_predictions)^2


# Display the performance metrics
cat("Mean Absolute Error (MAE):", mae, "\n")
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("R-squared (R²):", rsquared, "\n")

```

## 4

Now we are going to a fit a ridge regression model using `ridgereg()` function to the training dataset for different values of λ. The values of lambda chosen were: 1, 5, 10, 50, 200 and 10000.

```{r}
lambda <- c(1, 5, 10, 50, 200)
ridge_1 <- ridgereg(formula, train_set, lambda=1)
ridge_5 <- ridgereg(formula, train_set, lambda=5)
ridge_10 <- ridgereg(formula, train_set, lambda=10)
ridge_50 <- ridgereg(formula, train_set, lambda=50)
ridge_200 <- ridgereg(formula, train_set, lambda=200)
ridge_10000 <- ridgereg(formula, train_set, lambda=10000)
```

## 5

```{r}
# Divide the testdata (382 observations) into 10 different groups by indices.
k_fold_index <- 
  c(rep(1,38), rep(2,38), rep(3,38), rep(4,38), rep(5,38),
    rep(6,38), rep(7,38), rep(8,38), rep(9,39), rep(10,39))

# 10-fold cross validation with different lambda values evaluating with RMSE.
ten_fold_ridgereg <- function(k_fold_index,train_set,lambda){
  RMSE <- 1
  for(i in 1:10){
    index <- k_fold_index == i
    # 9 out of 10 sets are used to train the model
    train_data <- train_set[!index,]
    # 1 out of 10 set is used to evaluate the fit of the model
    test_data <- train_set[index, ]
    model <- ridgereg(formula, train_data, lambda=lambda)
    # 9 and 13 are variable "rad" and "lstat".
    RMSE[i] <- sqrt(sum((test_data$crim - model$predict(test_data[,c(9,13,12,2,8,3,14,5,11,4,10,7)]))^2))
  }
  mean_RMSE <- mean(RMSE)
  return(mean_RMSE)
}

ten_fold_ridgereg(k_fold_index,train_set,1)
ten_fold_ridgereg(k_fold_index,train_set,5)
ten_fold_ridgereg(k_fold_index,train_set,10)
ten_fold_ridgereg(k_fold_index,train_set,50)
ten_fold_ridgereg(k_fold_index,train_set,200)
ten_fold_ridgereg(k_fold_index,train_set,10000)
```

From the 10-fold cross validation the model with the lowest RMSE is the model with lambda=200.

## 6

We evaluate the performance of model with all predictor variables, the model with predictor varibles
chosen with forward selection, and a ridge regression model with the same predictor variables at the forward selection on the testset.

```{r}
lm_RMSE <- sqrt(sum((test_set$crim - predict(lin_mod, test_set))^2))
forward_RMSE <- sqrt(sum((test_set$crim - predict(forward_mod, test_set))^2))
ridge_RMSE <- sqrt(sum((test_set$crim - ridge_200$predict(test_set[, c(9,13,12,2,8,3,14,5,11,4,10,7)]) )^2))
lm_RMSE
forward_RMSE
ridge_RMSE

```

We get that the RMSE is a lot higher on the testset compared to the RMSE for the trainset. 
This implies that the models can be overfitted on trainset. Furthermore, the RMSE
is lowest for the forward-selection model. This makes sense since the forward-selection
has omitted the variable rm that did not appear to be able to predict crim. Interestingly
the ridge model was worse on testset than the forward-selection model, the ridge model
is supposed to regularise (be less overfitted on trainset) but it did not
make an improvement.



# 1.2.1 Predictive modeling of flight delays using `ridgereg()`

## 1

We read in the weather dataset and the flights dataset from the `nycflights13` package and select
variables we believe to have a predictive value.

```{r}
library(nycflights13)
 
flights <- nycflights13::flights
weather <- nycflights13::weather

# Nice variables
flights <- flights %>%  select(origin, year, month, day, hour,
                               dep_delay, arr_delay, distance)

weather <- weather %>%  select(origin, year, month,
                               day, hour, temp, wind_speed)
  
```


## 2

We add extra weather data (temp and wind_speed) from the weather dataset and create multiplicative interaction effects of temp and wind_speed.

We also make the assumption that the response variable is arrival delay.


```{r}

# merge flights with weather
merged_data <- flights %>%
  left_join(weather, by = c("origin", "year", "month", "day", "hour")) %>% 
  mutate(interactive_temp_wind = temp * wind_speed) %>% 
  na.omit() # remove Na's


```


## 3

We use the `caret` package to divide the flight dataset into three sets: test, train and validation (with the proportions 5%, 80% and 15%).


```{r}

# Set the seed for reproducibility
set.seed(123)

train_indices <- createDataPartition(merged_data$arr_delay, p = 0.80, list = FALSE)
train_set <- merged_data[train_indices, ]

test_val_set <- merged_data[-train_indices, ]


# 5% of 20% is 25%, thats why p = 0.25
test_indices <- createDataPartition(test_val_set$arr_delay, p = 0.25, list = FALSE)

test_set <- test_val_set[test_indices, ]
validation_set <- test_val_set[-test_indices, ]


```



## 4

We train ridge regressions models for different values of λ and evaluate the root mean squared error.
The formula for the model is: "arr_delay ~ dep_delay + distance + temp + wind_speed + interactive_temp_wind".

```{r}
index <-c(6,8,9,10,11)
predictors_subset <- colnames(merged_data)[index]
formula <- as.formula(paste("arr_delay ~", paste(predictors_subset, collapse = "+")))

ridge_1 <- ridgereg(formula, train_set, lambda=1)
ridge_5 <- ridgereg(formula, train_set, lambda=5)
ridge_10 <- ridgereg(formula, train_set, lambda=10)
ridge_50 <- ridgereg(formula, train_set, lambda=50)
ridge_200 <- ridgereg(formula, train_set, lambda=200)
ridge_10000 <- ridgereg(formula, train_set, lambda=10000)


resid_1 <- validation_set$arr_delay - ridge_1$predict(validation_set[index])
RMSE_1 <- sqrt(mean(resid_1^2)) 

resid_5 <- validation_set$arr_delay - ridge_5$predict(validation_set[index])
RMSE_5 <- sqrt(mean(resid_5^2)) 

resid_10 <- validation_set$arr_delay - ridge_10$predict(validation_set[index])
RMSE_10 <- sqrt(mean(resid_10^2)) 

resid_50 <- validation_set$arr_delay - ridge_50$predict(validation_set[index])
RMSE_50 <- sqrt(mean(resid_50^2)) 

resid_200 <- validation_set$arr_delay - ridge_200$predict(validation_set[index])
RMSE_200 <- sqrt(mean(resid_200^2)) 

resid_10000 <- validation_set$arr_delay - ridge_10000$predict(validation_set[index])
RMSE_10000 <- sqrt(mean(resid_10000^2)) 

RMSE <- c(RMSE_1, RMSE_5, RMSE_10, RMSE_50, RMSE_200, RMSE_10000)
RMSE


```

We found that the optimal value for λ is 200.


## 5

We predict the test set here.

```{r}
ridge_mod <- ridgereg(formula, test_set, lambda=200)
resid_mod <- test_set$arr_delay - ridge_mod$predict(test_set[index])
RMSE_mod <- sqrt(mean(resid_mod^2)) 

RMSE_mod  
```


The RMSE of ours predicted model is 17.99421.







