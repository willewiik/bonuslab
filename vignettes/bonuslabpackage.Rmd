---
title: "ridgereg"
author: "Duc Tran and William Wiik"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    fig_caption: yes
    toc: yes
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{ridgereg}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


# Introduction

This vignette show you how to do a simple prediction problem using 
`ridgereg()` function.
We are going to use the `caret` package and `ridgereg()` function to create a predictive model for the BostonHousing data found in the `mlbench` package.

# 1.2

## 1

First we divide the BostonHousing data into a test and training dataset using the
`caret` package (with the proportions 25% and 75%).


```{r}
library(mlbench)
library(caret)
library(dplyr)
library(bonuslabpackage)
library(leaps)

data(BostonHousing)
# Set the seed for reproducibility
set.seed(123)

training_indices <- createDataPartition(BostonHousing$crim, p = 0.75, list = FALSE)

train_set <- BostonHousing[training_indices, ]
test_set <- BostonHousing[-training_indices, ]


```


## 2

We fit a linear regression model with all predictor variables and a fit a linear regression model with forward selection of covariates on the training dataset.


```{r}
# Linear model with all predictor variables
lin_mod <- lm(crim ~ ., data = train_set)


# Linear model with only intercept
model_intercept <- lm("crim ~ 1", data = train_set) 

# Forward selection algorithm for linear model.
test_forward <- train(crim ~ ., data = train_set, 
                      method = "leapForward",
                      tuneGrid = data.frame(nvmax = 1:13)) # max all variables

# nvmax shows that 12 predictor variables were the max.
test_forward
```


The forward selection algorithm shows that 12 predictor variables is the best.
The variables used and in what order are presented below.

```{r}
# Algorithm shows that the best model is to add predictor in the order as follows:
# rad, lstat, b, and zn.
summary(test_forward)
```


The best model is to add the predictor variables in the order: rad, lstat, b, zn, dis, indus, medv, nox, ptratio, chas, tax, and age. The only variable not used is rm.
We estimate this linear model and compare it with the model with only the intercept by using AIC.

```{r}
formula <- as.formula("crim ~ rad + lstat + b + zn + dis + indus + 
                    medv + nox + ptratio + chas + tax + age")

# Forward-selection model used.
forward_mod <- lm(formula, train_set, method="lm")

# We use AIC to compare the model with only intercept with the forward-selection model.
AIC(model_intercept)
AIC(forward_mod)

```

From the output the AIC for the forward selection model is lower than the model with intercept.
This implies that the forward selection model is better with AIC as a criterion.


## 3

Now we evaluate the performance of this model on the training dataset.

```{r}


# Predict using the selected model on the training dataset
train_predictions <- predict(forward_mod, newdata = train_set)


mae <- mean(abs(train_set$crim - train_predictions))
mse <- mean((train_set$crim - train_predictions)^2)
rmse <- sqrt(mse)
rsquared <- cor(train_set$crim, train_predictions)^2


# Display the performance metrics
cat("Mean Absolute Error (MAE):", mae, "\n")
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("R-squared (R²):", rsquared, "\n")

```

## 4

Now we are going to a fit a ridge regression model using `ridgereg()` function to the training dataset for different values of λ. The values of lambda chosen were: 1, 5, 10, 50, 200 and 10000.

```{r}
lambda <- c(1, 5, 10, 50, 200)
ridge_1 <- ridgereg(formula, train_set, lambda=1)
ridge_5 <- ridgereg(formula, train_set, lambda=5)
ridge_10 <- ridgereg(formula, train_set, lambda=10)
ridge_50 <- ridgereg(formula, train_set, lambda=50)
ridge_200 <- ridgereg(formula, train_set, lambda=200)
ridge_10000 <- ridgereg(formula, train_set, lambda=10000)
```

## 5

```{r}
# Divide the testdata (382 observations) into 10 different groups by indices.
k_fold_index <- 
  c(rep(1,38), rep(2,38), rep(3,38), rep(4,38), rep(5,38),
    rep(6,38), rep(7,38), rep(8,38), rep(9,39), rep(10,39))

# 10-fold cross validation with different lambda values evaluating with RMSE.
ten_fold_ridgereg <- function(k_fold_index,train_set,lambda){
  RMSE <- 1
  for(i in 1:10){
    index <- k_fold_index == i
    # 9 out of 10 sets are used to train the model
    train_data <- train_set[!index,]
    # 1 out of 10 set is used to evaluate the fit of the model
    test_data <- train_set[index, ]
    model <- ridgereg(formula, train_data, lambda=lambda)
    # 9 and 13 are variable "rad" and "lstat".
    RMSE[i] <- sqrt(sum((test_data$crim - model$predict(test_data[,c(9,13,12,2,8,3,14,5,11,4,10,7)]))^2))
  }
  mean_RMSE <- mean(RMSE)
  return(mean_RMSE)
}

ten_fold_ridgereg(k_fold_index,train_set,1)
ten_fold_ridgereg(k_fold_index,train_set,5)
ten_fold_ridgereg(k_fold_index,train_set,10)
ten_fold_ridgereg(k_fold_index,train_set,50)
ten_fold_ridgereg(k_fold_index,train_set,200)
ten_fold_ridgereg(k_fold_index,train_set,10000)
```

From the 10-fold cross validation the model with the lowest RMSE is the model with lambda=200.

## 6

We evaluate the performance of model with all predictor variables, the model with predictor varibles
chosen with forward selection, and a ridge regression model with the same predictor variables at the forward selection on the testset.

```{r}
lm_RMSE <- sqrt(sum((test_set$crim - predict(lin_mod, test_set))^2))
forward_RMSE <- sqrt(sum((test_set$crim - predict(forward_mod, test_set))^2))
ridge_RMSE <- sqrt(sum((test_set$crim - ridge_200$predict(test_set[, c(9,13,12,2,8,3,14,5,11,4,10,7)]) )^2))
lm_RMSE
forward_RMSE
ridge_RMSE

```

We get that the RMSE is a lot higher on the testset compared to the RMSE for the trainset. 
This implies that the models can be overfitted on trainset. Furthermore, the RMSE
is lowest for the forward-selection model. This makes sense since the forward-selection
has omitted the variable rm that did not appear to be able to predict crim. Interestingly
the ridge model was worse on testset than the forward-selection model, the ridge model
is supposed to regularise (be less overfitted on trainset) but it did not
make an improvement.


